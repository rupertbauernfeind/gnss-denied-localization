{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d89a67a",
   "metadata": {},
   "source": [
    "# 15 - SIFT + SfM Iterative Test Localization (Submission)\n",
    "\n",
    "This notebook is based on the core method from notebook 12 **before** the feature-focus sweep.\n",
    "\n",
    "Goal:\n",
    "1. Build train-test batches/segments from IDs.\n",
    "2. Estimate relative motion iteratively with SIFT + affine/SfM cues.\n",
    "3. Use optional segment closure to next train anchor to reduce drift.\n",
    "4. Export `submission.csv` for all test frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77287a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 130\n",
    "np.set_printoptions(precision=4, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c50299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks + paths + config\n",
    "if not hasattr(cv2, 'SIFT_create'):\n",
    "    raise RuntimeError('OpenCV build has no SIFT. Install opencv-contrib-python.')\n",
    "\n",
    "CANDIDATE_ROOTS = [Path.cwd(), Path.cwd().parent, Path('.'), Path('..')]\n",
    "PROJECT_ROOT = None\n",
    "_seen = set()\n",
    "for cand in CANDIDATE_ROOTS:\n",
    "    try:\n",
    "        root = cand.resolve()\n",
    "    except Exception:\n",
    "        continue\n",
    "    key = str(root)\n",
    "    if key in _seen:\n",
    "        continue\n",
    "    _seen.add(key)\n",
    "    if (root / 'data' / 'data').exists():\n",
    "        PROJECT_ROOT = root\n",
    "        break\n",
    "if PROJECT_ROOT is None:\n",
    "    raise FileNotFoundError('Could not find project root containing data/data.')\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / 'data' / 'data'\n",
    "TRAIN_IMG_DIR = DATA_ROOT / 'train_data' / 'train_images'\n",
    "TEST_IMG_DIR = DATA_ROOT / 'test_data' / 'test_images'\n",
    "TRAIN_POS_CSV = DATA_ROOT / 'train_data' / 'train_pos.csv'\n",
    "TRAIN_CAM_CSV = DATA_ROOT / 'train_data' / 'train_cam.csv'\n",
    "TEST_CAM_CSV = DATA_ROOT / 'test_data' / 'test_cam.csv'\n",
    "MAP_PATH = DATA_ROOT / 'map.png'\n",
    "\n",
    "# Runtime\n",
    "IMAGE_MAX_SIDE = 1000\n",
    "PREPROCESS_MODE = 'gray_clahe'  # ['gray', 'gray_clahe', 'gray_denoise_clahe']\n",
    "\n",
    "# SIFT settings (notebook 12 style: balanced)\n",
    "SIFT_CFG = {\n",
    "    'nfeatures': 6500,\n",
    "    'contrast_thr': 0.03,\n",
    "    'edge_thr': 12,\n",
    "    'sigma': 1.6,\n",
    "    'ratio_thr': 0.78,\n",
    "}\n",
    "\n",
    "# Geometry\n",
    "AFFINE_RANSAC_THR = 3.0\n",
    "HOMOGRAPHY_RANSAC_THR = 4.0\n",
    "ESSENTIAL_RANSAC_THR = 1.5\n",
    "MIN_MATCHES_FOR_GEOM = 8\n",
    "MIN_AFFINE_INLIERS_FOR_MODEL = 10\n",
    "\n",
    "# Batch/segment behavior\n",
    "AUTO_SWITCH_TO_NEXT_ANCHOR = True\n",
    "ANCHOR_SWITCH_MIN_INLIERS = 16\n",
    "CLOSURE_MIN_INLIERS = 12\n",
    "\n",
    "# Calibration behavior\n",
    "CALIB_USE_BIDIRECTIONAL = True\n",
    "CALIB_MAX_PAIRS = 260  # speed/quality tradeoff\n",
    "ROBUST_ITERS = 5\n",
    "HUBER_DELTA = 35.0\n",
    "RIDGE = 1e-4\n",
    "\n",
    "# Output\n",
    "SUBMISSION_PATH = PROJECT_ROOT / 'build' / 'submission.csv'\n",
    "ALT_SUBMISSION_PATH = PROJECT_ROOT / 'build' / 'submission_15_sift_sfm.csv'\n",
    "\n",
    "print('project_root:', PROJECT_ROOT)\n",
    "print('train_img_dir:', TRAIN_IMG_DIR)\n",
    "print('test_img_dir:', TEST_IMG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "train_pos_df = pd.read_csv(TRAIN_POS_CSV)\n",
    "train_cam_df = pd.read_csv(TRAIN_CAM_CSV)\n",
    "test_cam_df = pd.read_csv(TEST_CAM_CSV)\n",
    "\n",
    "for c in ['id', 'x_pixel', 'y_pixel']:\n",
    "    if c not in train_pos_df.columns:\n",
    "        raise KeyError(f'Missing column in train_pos.csv: {c}')\n",
    "for c in ['id', 'fx', 'fy', 'cx', 'cy']:\n",
    "    if c not in train_cam_df.columns:\n",
    "        raise KeyError(f'Missing column in train_cam.csv: {c}')\n",
    "    if c not in test_cam_df.columns:\n",
    "        raise KeyError(f'Missing column in test_cam.csv: {c}')\n",
    "\n",
    "train_df = train_cam_df.merge(train_pos_df, on='id', how='inner').copy()\n",
    "train_df['id'] = train_df['id'].astype(int)\n",
    "train_df = train_df.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "train_cam_df = train_cam_df.copy()\n",
    "train_cam_df['id'] = train_cam_df['id'].astype(int)\n",
    "test_cam_df = test_cam_df.copy()\n",
    "test_cam_df['id'] = test_cam_df['id'].astype(int)\n",
    "\n",
    "train_ids = sorted(train_df['id'].astype(int).tolist())\n",
    "test_ids = sorted(test_cam_df['id'].astype(int).tolist())\n",
    "\n",
    "train_pos_map = {\n",
    "    int(r['id']): np.array([float(r['x_pixel']), float(r['y_pixel'])], dtype=np.float64)\n",
    "    for _, r in train_df.iterrows()\n",
    "}\n",
    "train_cam_map = {int(r['id']): r for _, r in train_cam_df.iterrows()}\n",
    "test_cam_map = {int(r['id']): r for _, r in test_cam_df.iterrows()}\n",
    "\n",
    "train_id_set = set(train_ids)\n",
    "test_id_set = set(test_ids)\n",
    "\n",
    "\n",
    "def build_test_segments(ids: List[int]) -> List[Tuple[int, int]]:\n",
    "    if len(ids) == 0:\n",
    "        return []\n",
    "    segs: List[Tuple[int, int]] = []\n",
    "    s = ids[0]\n",
    "    p = ids[0]\n",
    "    for x in ids[1:]:\n",
    "        if x == p + 1:\n",
    "            p = x\n",
    "        else:\n",
    "            segs.append((s, p))\n",
    "            s = x\n",
    "            p = x\n",
    "    segs.append((s, p))\n",
    "    return segs\n",
    "\n",
    "\n",
    "test_segments = build_test_segments(test_ids)\n",
    "\n",
    "seg_rows = []\n",
    "for i, (s, e) in enumerate(test_segments):\n",
    "    prev_train = max([t for t in train_ids if t < s], default=None)\n",
    "    next_train = min([t for t in train_ids if t > e], default=None)\n",
    "    seg_rows.append({\n",
    "        'segment_idx': i,\n",
    "        'start_id': s,\n",
    "        'end_id': e,\n",
    "        'length': e - s + 1,\n",
    "        'prev_train': prev_train,\n",
    "        'next_train': next_train,\n",
    "    })\n",
    "\n",
    "segments_df = pd.DataFrame(seg_rows)\n",
    "print('train frames:', len(train_ids), 'test frames:', len(test_ids), 'test segments:', len(test_segments))\n",
    "display(segments_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64252ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities: image loading, preprocessing, intrinsics\n",
    "_IMG_CACHE: Dict[Tuple[int, Optional[int]], Tuple[np.ndarray, float]] = {}\n",
    "_PREPROC_CACHE: Dict[Tuple[int, Optional[int], str], Tuple[np.ndarray, float]] = {}\n",
    "_FEATURE_CACHE: Dict[Tuple[int, Optional[int], str, Tuple], Tuple[List[cv2.KeyPoint], Optional[np.ndarray], np.ndarray, np.ndarray, float]] = {}\n",
    "\n",
    "\n",
    "def infer_source(image_id: int) -> str:\n",
    "    if int(image_id) in train_id_set:\n",
    "        return 'train'\n",
    "    if int(image_id) in test_id_set:\n",
    "        return 'test'\n",
    "    raise KeyError(f'Image id {image_id} in neither train nor test metadata.')\n",
    "\n",
    "\n",
    "def resolve_image_path(image_id: int) -> Path:\n",
    "    src = infer_source(int(image_id))\n",
    "    base_dir = TRAIN_IMG_DIR if src == 'train' else TEST_IMG_DIR\n",
    "    stems = [f'{int(image_id):04d}', str(int(image_id))]\n",
    "    exts = ['.JPG', '.jpg', '.jpeg', '.JPEG', '.png', '.PNG']\n",
    "    for st in stems:\n",
    "        for ext in exts:\n",
    "            p = base_dir / f'{st}{ext}'\n",
    "            if p.exists():\n",
    "                return p\n",
    "    raise FileNotFoundError(f'Image not found for id={image_id} in {base_dir}')\n",
    "\n",
    "\n",
    "def load_image_rgb(image_id: int, max_side: Optional[int]) -> Tuple[np.ndarray, float]:\n",
    "    key = (int(image_id), max_side)\n",
    "    if key in _IMG_CACHE:\n",
    "        return _IMG_CACHE[key]\n",
    "\n",
    "    p = resolve_image_path(int(image_id))\n",
    "    bgr = cv2.imread(str(p), cv2.IMREAD_COLOR)\n",
    "    if bgr is None:\n",
    "        raise RuntimeError(f'Cannot read image: {p}')\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    scale = 1.0\n",
    "    if max_side is not None:\n",
    "        h, w = rgb.shape[:2]\n",
    "        m = max(h, w)\n",
    "        if m > int(max_side):\n",
    "            scale = float(max_side) / float(m)\n",
    "            nw = max(32, int(round(w * scale)))\n",
    "            nh = max(32, int(round(h * scale)))\n",
    "            rgb = cv2.resize(rgb, (nw, nh), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    _IMG_CACHE[key] = (rgb, scale)\n",
    "    return rgb, scale\n",
    "\n",
    "\n",
    "def preprocess_for_sift(img_rgb: np.ndarray, mode: str) -> np.ndarray:\n",
    "    g = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    if mode == 'gray':\n",
    "        return g\n",
    "    if mode == 'gray_clahe':\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        return clahe.apply(g)\n",
    "    if mode == 'gray_denoise_clahe':\n",
    "        g2 = cv2.bilateralFilter(g, d=7, sigmaColor=45, sigmaSpace=45)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        return clahe.apply(g2)\n",
    "    raise KeyError(f'Unknown preprocess mode: {mode}')\n",
    "\n",
    "\n",
    "def load_preprocessed_gray(image_id: int, max_side: Optional[int], mode: str) -> Tuple[np.ndarray, float]:\n",
    "    key = (int(image_id), max_side, str(mode))\n",
    "    if key in _PREPROC_CACHE:\n",
    "        return _PREPROC_CACHE[key]\n",
    "    rgb, scale = load_image_rgb(int(image_id), max_side=max_side)\n",
    "    gray = preprocess_for_sift(rgb, mode=mode)\n",
    "    _PREPROC_CACHE[key] = (gray, scale)\n",
    "    return gray, scale\n",
    "\n",
    "\n",
    "def get_cam_row(image_id: int) -> pd.Series:\n",
    "    iid = int(image_id)\n",
    "    if iid in train_cam_map:\n",
    "        return train_cam_map[iid]\n",
    "    if iid in test_cam_map:\n",
    "        return test_cam_map[iid]\n",
    "    raise KeyError(f'No intrinsics row for id={image_id}')\n",
    "\n",
    "\n",
    "def scaled_K(image_id: int, image_scale: float) -> np.ndarray:\n",
    "    row = get_cam_row(int(image_id))\n",
    "    fx = float(row['fx']) * float(image_scale)\n",
    "    fy = float(row['fy']) * float(image_scale)\n",
    "    cx = float(row['cx']) * float(image_scale)\n",
    "    cy = float(row['cy']) * float(image_scale)\n",
    "    return np.array([[fx, 0.0, cx], [0.0, fy, cy], [0.0, 0.0, 1.0]], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair motion estimation (SIFT + affine + optional SfM)\n",
    "@dataclass\n",
    "class PairMotion:\n",
    "    id0: int\n",
    "    id1: int\n",
    "    success: bool\n",
    "    reason: str\n",
    "    matches: int\n",
    "    affine_inliers: int\n",
    "    homography_inliers: int\n",
    "    sfm_inliers: int\n",
    "    affine_M_1to0: Optional[np.ndarray]\n",
    "    H_1to0: Optional[np.ndarray]\n",
    "    tx: float\n",
    "    ty: float\n",
    "    rot_deg: float\n",
    "    scale: float\n",
    "    sfm_t: Optional[np.ndarray]\n",
    "\n",
    "\n",
    "def get_features_for_id(image_id: int, cfg: Dict[str, float]):\n",
    "    cfg_key = (\n",
    "        int(cfg['nfeatures']),\n",
    "        float(cfg['contrast_thr']),\n",
    "        float(cfg['edge_thr']),\n",
    "        float(cfg['sigma']),\n",
    "        str(PREPROCESS_MODE),\n",
    "    )\n",
    "    key = (int(image_id), IMAGE_MAX_SIDE, PREPROCESS_MODE, cfg_key)\n",
    "    if key in _FEATURE_CACHE:\n",
    "        return _FEATURE_CACHE[key]\n",
    "\n",
    "    gray, scale = load_preprocessed_gray(int(image_id), IMAGE_MAX_SIDE, PREPROCESS_MODE)\n",
    "    K = scaled_K(int(image_id), image_scale=scale)\n",
    "\n",
    "    sift = cv2.SIFT_create(\n",
    "        nfeatures=int(cfg['nfeatures']),\n",
    "        contrastThreshold=float(cfg['contrast_thr']),\n",
    "        edgeThreshold=float(cfg['edge_thr']),\n",
    "        sigma=float(cfg['sigma']),\n",
    "    )\n",
    "    kps, desc = sift.detectAndCompute(gray, None)\n",
    "    if kps is None:\n",
    "        kps = []\n",
    "\n",
    "    _FEATURE_CACHE[key] = (kps, desc, gray, K, scale)\n",
    "    return _FEATURE_CACHE[key]\n",
    "\n",
    "\n",
    "_PAIR_MOTION_CACHE: Dict[Tuple[int, int, Tuple], PairMotion] = {}\n",
    "\n",
    "\n",
    "def estimate_pair_motion(id0: int, id1: int, cfg: Dict[str, float]) -> PairMotion:\n",
    "    cfg_key = (\n",
    "        int(cfg['nfeatures']),\n",
    "        float(cfg['contrast_thr']),\n",
    "        float(cfg['edge_thr']),\n",
    "        float(cfg['sigma']),\n",
    "        float(cfg['ratio_thr']),\n",
    "    )\n",
    "    cache_key = (int(id0), int(id1), cfg_key)\n",
    "    if cache_key in _PAIR_MOTION_CACHE:\n",
    "        return _PAIR_MOTION_CACHE[cache_key]\n",
    "\n",
    "    k0, d0, g0, K0, _ = get_features_for_id(int(id0), cfg)\n",
    "    k1, d1, g1, K1, _ = get_features_for_id(int(id1), cfg)\n",
    "\n",
    "    if d0 is None or d1 is None or len(k0) < 2 or len(k1) < 2:\n",
    "        out = PairMotion(int(id0), int(id1), False, 'no_descriptors', 0, 0, 0, 0, None, None, np.nan, np.nan, np.nan, np.nan, None)\n",
    "        _PAIR_MOTION_CACHE[cache_key] = out\n",
    "        return out\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "    knn = bf.knnMatch(d0, d1, k=2)\n",
    "\n",
    "    good = []\n",
    "    ratio_thr = float(cfg['ratio_thr'])\n",
    "    for pair in knn:\n",
    "        if len(pair) < 2:\n",
    "            continue\n",
    "        m, n = pair\n",
    "        if m.distance < ratio_thr * n.distance:\n",
    "            good.append(m)\n",
    "\n",
    "    if len(good) < MIN_MATCHES_FOR_GEOM:\n",
    "        out = PairMotion(int(id0), int(id1), False, 'few_matches', len(good), 0, 0, 0, None, None, np.nan, np.nan, np.nan, np.nan, None)\n",
    "        _PAIR_MOTION_CACHE[cache_key] = out\n",
    "        return out\n",
    "\n",
    "    pts0 = np.array([k0[m.queryIdx].pt for m in good], dtype=np.float32)\n",
    "    pts1 = np.array([k1[m.trainIdx].pt for m in good], dtype=np.float32)\n",
    "\n",
    "    M, inl_aff = cv2.estimateAffinePartial2D(\n",
    "        pts1,\n",
    "        pts0,\n",
    "        method=cv2.RANSAC,\n",
    "        ransacReprojThreshold=float(AFFINE_RANSAC_THR),\n",
    "        maxIters=20000,\n",
    "        confidence=0.999,\n",
    "        refineIters=50,\n",
    "    )\n",
    "    if inl_aff is not None:\n",
    "        inl_aff = inl_aff.ravel().astype(bool)\n",
    "        n_inl_aff = int(inl_aff.sum())\n",
    "    else:\n",
    "        inl_aff = np.zeros((len(good),), dtype=bool)\n",
    "        n_inl_aff = 0\n",
    "\n",
    "    if M is not None:\n",
    "        rot_deg = float(np.degrees(np.arctan2(M[1, 0], M[0, 0])))\n",
    "        scale = float(np.sqrt(M[0, 0] ** 2 + M[1, 0] ** 2))\n",
    "        tx = float(M[0, 2])\n",
    "        ty = float(M[1, 2])\n",
    "    else:\n",
    "        rot_deg = np.nan\n",
    "        scale = np.nan\n",
    "        tx = np.nan\n",
    "        ty = np.nan\n",
    "\n",
    "    method = cv2.USAC_MAGSAC if hasattr(cv2, 'USAC_MAGSAC') else cv2.RANSAC\n",
    "    H, inl_h = cv2.findHomography(\n",
    "        pts1,\n",
    "        pts0,\n",
    "        method=method,\n",
    "        ransacReprojThreshold=float(HOMOGRAPHY_RANSAC_THR),\n",
    "        maxIters=20000,\n",
    "        confidence=0.999,\n",
    "    )\n",
    "    if inl_h is not None:\n",
    "        n_inl_h = int(inl_h.ravel().astype(bool).sum())\n",
    "    else:\n",
    "        n_inl_h = 0\n",
    "\n",
    "    sfm_t = None\n",
    "    n_inl_pose = 0\n",
    "    if len(good) >= 8:\n",
    "        pts0n = cv2.undistortPoints(pts0.reshape(-1, 1, 2), K0, None).reshape(-1, 2)\n",
    "        pts1n = cv2.undistortPoints(pts1.reshape(-1, 1, 2), K1, None).reshape(-1, 2)\n",
    "        E, inl_e = cv2.findEssentialMat(\n",
    "            pts0n,\n",
    "            pts1n,\n",
    "            cameraMatrix=np.eye(3),\n",
    "            method=cv2.RANSAC,\n",
    "            prob=0.999,\n",
    "            threshold=float(ESSENTIAL_RANSAC_THR),\n",
    "        )\n",
    "        if E is not None and inl_e is not None and int(inl_e.sum()) >= 8:\n",
    "            _, R, t, inl_pose = cv2.recoverPose(\n",
    "                E,\n",
    "                pts0n,\n",
    "                pts1n,\n",
    "                cameraMatrix=np.eye(3),\n",
    "                mask=inl_e.astype(np.uint8).reshape(-1, 1),\n",
    "            )\n",
    "            sfm_t = t.reshape(3).astype(np.float64)\n",
    "            if inl_pose is not None:\n",
    "                n_inl_pose = int(inl_pose.ravel().astype(bool).sum())\n",
    "\n",
    "    success = (M is not None) and (n_inl_aff >= MIN_AFFINE_INLIERS_FOR_MODEL)\n",
    "    reason = 'ok' if success else 'weak_affine'\n",
    "\n",
    "    out = PairMotion(\n",
    "        id0=int(id0),\n",
    "        id1=int(id1),\n",
    "        success=bool(success),\n",
    "        reason=reason,\n",
    "        matches=int(len(good)),\n",
    "        affine_inliers=int(n_inl_aff),\n",
    "        homography_inliers=int(n_inl_h),\n",
    "        sfm_inliers=int(n_inl_pose),\n",
    "        affine_M_1to0=M,\n",
    "        H_1to0=H,\n",
    "        tx=float(tx),\n",
    "        ty=float(ty),\n",
    "        rot_deg=float(rot_deg),\n",
    "        scale=float(scale),\n",
    "        sfm_t=sfm_t,\n",
    "    )\n",
    "    _PAIR_MOTION_CACHE[cache_key] = out\n",
    "    return out\n",
    "\n",
    "\n",
    "def motion_to_feature_vector(m: PairMotion) -> np.ndarray:\n",
    "    sfm_tx, sfm_ty, sfm_tz = 0.0, 0.0, 0.0\n",
    "    if m.sfm_t is not None:\n",
    "        t = m.sfm_t.astype(np.float64)\n",
    "        tn = float(np.linalg.norm(t))\n",
    "        if tn > 1e-12:\n",
    "            t = t / tn\n",
    "            sfm_tx, sfm_ty, sfm_tz = float(t[0]), float(t[1]), float(t[2])\n",
    "\n",
    "    rot_rad = 0.0 if not np.isfinite(m.rot_deg) else float(np.deg2rad(m.rot_deg))\n",
    "    scale_log = 0.0 if (not np.isfinite(m.scale) or m.scale <= 1e-8) else float(np.log(m.scale))\n",
    "    tx = 0.0 if not np.isfinite(m.tx) else float(m.tx)\n",
    "    ty = 0.0 if not np.isfinite(m.ty) else float(m.ty)\n",
    "\n",
    "    return np.array([tx, ty, rot_rad, scale_log, sfm_tx, sfm_ty, sfm_tz], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust linear mapping: motion features -> map delta\n",
    "@dataclass\n",
    "class MotionLinearModel:\n",
    "    beta: np.ndarray  # shape (d+1, 2)\n",
    "    feature_names: List[str]\n",
    "    med_err: float\n",
    "    mean_err: float\n",
    "    used_pairs: int\n",
    "\n",
    "\n",
    "def fit_robust_linear_model(\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    base_w: np.ndarray,\n",
    "    robust_iters: int = 5,\n",
    "    huber_delta: float = 35.0,\n",
    "    ridge: float = 1e-4,\n",
    ") -> np.ndarray:\n",
    "    n, d = X.shape\n",
    "    Xb = np.hstack([X, np.ones((n, 1), dtype=np.float64)])\n",
    "\n",
    "    w = np.clip(base_w.astype(np.float64), 1e-6, None).copy()\n",
    "\n",
    "    for _ in range(int(robust_iters)):\n",
    "        W = np.sqrt(w).reshape(-1, 1)\n",
    "        Xw = Xb * W\n",
    "        Yw = Y * W\n",
    "\n",
    "        XtX = Xw.T @ Xw\n",
    "        XtY = Xw.T @ Yw\n",
    "        XtX = XtX + float(ridge) * np.eye(XtX.shape[0], dtype=np.float64)\n",
    "\n",
    "        beta = np.linalg.solve(XtX, XtY)\n",
    "\n",
    "        pred = Xb @ beta\n",
    "        err = np.linalg.norm(Y - pred, axis=1)\n",
    "        hub = np.ones_like(err)\n",
    "        bad = err > float(huber_delta)\n",
    "        hub[bad] = float(huber_delta) / np.clip(err[bad], 1e-6, None)\n",
    "        w = np.clip(base_w, 1e-6, None) * hub\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "def predict_delta(model: MotionLinearModel, m: PairMotion) -> np.ndarray:\n",
    "    x = motion_to_feature_vector(m)\n",
    "    xb = np.concatenate([x, np.array([1.0], dtype=np.float64)], axis=0)\n",
    "    y = xb @ model.beta\n",
    "    return y.astype(np.float64)\n",
    "\n",
    "\n",
    "# Build directed calibration pairs from consecutive train ids\n",
    "calib_pairs: List[Tuple[int, int]] = []\n",
    "for i in range(len(train_ids) - 1):\n",
    "    a = int(train_ids[i])\n",
    "    b = int(train_ids[i + 1])\n",
    "    if b != a + 1:\n",
    "        continue\n",
    "    calib_pairs.append((a, b))\n",
    "    if CALIB_USE_BIDIRECTIONAL:\n",
    "        calib_pairs.append((b, a))\n",
    "\n",
    "if CALIB_MAX_PAIRS is not None and len(calib_pairs) > int(CALIB_MAX_PAIRS):\n",
    "    idx = np.linspace(0, len(calib_pairs) - 1, int(CALIB_MAX_PAIRS)).astype(int)\n",
    "    calib_pairs = [calib_pairs[i] for i in idx]\n",
    "\n",
    "rows = []\n",
    "X_list = []\n",
    "Y_list = []\n",
    "W_list = []\n",
    "\n",
    "for id0, id1 in calib_pairs:\n",
    "    m = estimate_pair_motion(id0, id1, SIFT_CFG)\n",
    "    gt = train_pos_map[id1] - train_pos_map[id0]\n",
    "\n",
    "    rows.append({\n",
    "        'id0': id0,\n",
    "        'id1': id1,\n",
    "        'matches': m.matches,\n",
    "        'affine_inliers': m.affine_inliers,\n",
    "        'sfm_inliers': m.sfm_inliers,\n",
    "        'success': m.success,\n",
    "        'gt_dx': float(gt[0]),\n",
    "        'gt_dy': float(gt[1]),\n",
    "        'tx': m.tx,\n",
    "        'ty': m.ty,\n",
    "        'rot_deg': m.rot_deg,\n",
    "        'scale': m.scale,\n",
    "    })\n",
    "\n",
    "    if m.success:\n",
    "        X_list.append(motion_to_feature_vector(m))\n",
    "        Y_list.append(gt.astype(np.float64))\n",
    "        W_list.append(max(1.0, float(m.affine_inliers)))\n",
    "\n",
    "calib_df = pd.DataFrame(rows)\n",
    "print('calibration directed pairs:', len(calib_pairs), 'usable for model:', len(X_list))\n",
    "display(calib_df.head(20))\n",
    "\n",
    "if len(X_list) < 20:\n",
    "    raise RuntimeError('Too few usable calibration pairs. Try looser SIFT thresholds or lower MIN_AFFINE_INLIERS_FOR_MODEL.')\n",
    "\n",
    "X = np.vstack(X_list)\n",
    "Y = np.vstack(Y_list)\n",
    "W = np.array(W_list, dtype=np.float64)\n",
    "\n",
    "beta = fit_robust_linear_model(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    base_w=W,\n",
    "    robust_iters=ROBUST_ITERS,\n",
    "    huber_delta=HUBER_DELTA,\n",
    "    ridge=RIDGE,\n",
    ")\n",
    "\n",
    "feature_names = ['tx', 'ty', 'rot_rad', 'log_scale', 'sfm_tx', 'sfm_ty', 'sfm_tz', 'bias']\n",
    "model = MotionLinearModel(beta=beta, feature_names=feature_names, med_err=np.nan, mean_err=np.nan, used_pairs=int(len(X)))\n",
    "\n",
    "pred_calib = np.hstack([X, np.ones((X.shape[0], 1), dtype=np.float64)]) @ beta\n",
    "calib_err = np.linalg.norm(Y - pred_calib, axis=1)\n",
    "model.med_err = float(np.median(calib_err))\n",
    "model.mean_err = float(np.mean(calib_err))\n",
    "\n",
    "coef_df = pd.DataFrame(\n",
    "    model.beta,\n",
    "    index=feature_names,\n",
    "    columns=['dx_coef', 'dy_coef'],\n",
    ")\n",
    "print(f'calibration error: median={model.med_err:.2f}px mean={model.mean_err:.2f}px')\n",
    "display(coef_df)\n",
    "\n",
    "# fallback step from train GT consecutive deltas\n",
    "train_step_deltas = []\n",
    "for i in range(len(train_ids) - 1):\n",
    "    a = int(train_ids[i])\n",
    "    b = int(train_ids[i + 1])\n",
    "    if b == a + 1:\n",
    "        train_step_deltas.append(train_pos_map[b] - train_pos_map[a])\n",
    "train_step_deltas = np.array(train_step_deltas, dtype=np.float64)\n",
    "fallback_forward = np.median(train_step_deltas, axis=0)\n",
    "fallback_backward = -fallback_forward\n",
    "print('fallback_forward delta:', np.round(fallback_forward, 3))\n",
    "print('fallback_backward delta:', np.round(fallback_backward, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66636f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test batch plans\n",
    "@dataclass\n",
    "class SegmentPlan:\n",
    "    segment_idx: int\n",
    "    start_id: int\n",
    "    end_id: int\n",
    "    length: int\n",
    "    anchor_train_id: int\n",
    "    direction: str  # 'forward' or 'backward'\n",
    "    ordered_test_ids: List[int]\n",
    "    closing_train_id: Optional[int]\n",
    "    prev_train_id: Optional[int]\n",
    "    next_train_id: Optional[int]\n",
    "    entry_inliers_forward: Optional[int]\n",
    "    entry_inliers_backward: Optional[int]\n",
    "\n",
    "\n",
    "def make_segment_plan(seg_idx: int, s: int, e: int) -> SegmentPlan:\n",
    "    prev_train = max([t for t in train_ids if t < s], default=None)\n",
    "    next_train = min([t for t in train_ids if t > e], default=None)\n",
    "\n",
    "    # default rule from request\n",
    "    if prev_train is None:\n",
    "        # first test frames 1..12: anchor at 13 and run backward\n",
    "        if next_train is None:\n",
    "            raise RuntimeError(f'Segment {s}-{e} has no train anchor on either side.')\n",
    "        anchor = int(next_train)\n",
    "        direction = 'backward'\n",
    "        ordered = list(range(int(e), int(s) - 1, -1))\n",
    "        closing = prev_train\n",
    "    else:\n",
    "        anchor = int(prev_train)\n",
    "        direction = 'forward'\n",
    "        ordered = list(range(int(s), int(e) + 1))\n",
    "        closing = next_train\n",
    "\n",
    "    inl_fwd = None\n",
    "    inl_bwd = None\n",
    "\n",
    "    # optional rescue for hard segments: choose side with better entry match\n",
    "    if AUTO_SWITCH_TO_NEXT_ANCHOR and (prev_train is not None) and (next_train is not None):\n",
    "        m_fwd = estimate_pair_motion(int(prev_train), int(s), SIFT_CFG)\n",
    "        m_bwd = estimate_pair_motion(int(next_train), int(e), SIFT_CFG)\n",
    "        inl_fwd = int(m_fwd.affine_inliers)\n",
    "        inl_bwd = int(m_bwd.affine_inliers)\n",
    "\n",
    "        if (inl_fwd < ANCHOR_SWITCH_MIN_INLIERS) and (inl_bwd >= ANCHOR_SWITCH_MIN_INLIERS) and (inl_bwd > inl_fwd):\n",
    "            anchor = int(next_train)\n",
    "            direction = 'backward'\n",
    "            ordered = list(range(int(e), int(s) - 1, -1))\n",
    "            closing = prev_train\n",
    "\n",
    "    return SegmentPlan(\n",
    "        segment_idx=int(seg_idx),\n",
    "        start_id=int(s),\n",
    "        end_id=int(e),\n",
    "        length=int(e - s + 1),\n",
    "        anchor_train_id=int(anchor),\n",
    "        direction=str(direction),\n",
    "        ordered_test_ids=[int(x) for x in ordered],\n",
    "        closing_train_id=int(closing) if closing is not None else None,\n",
    "        prev_train_id=int(prev_train) if prev_train is not None else None,\n",
    "        next_train_id=int(next_train) if next_train is not None else None,\n",
    "        entry_inliers_forward=inl_fwd,\n",
    "        entry_inliers_backward=inl_bwd,\n",
    "    )\n",
    "\n",
    "\n",
    "segment_plans: List[SegmentPlan] = []\n",
    "for i, (s, e) in enumerate(test_segments):\n",
    "    segment_plans.append(make_segment_plan(i, s, e))\n",
    "\n",
    "plan_rows = []\n",
    "for p in segment_plans:\n",
    "    plan_rows.append({\n",
    "        'segment_idx': p.segment_idx,\n",
    "        'start_id': p.start_id,\n",
    "        'end_id': p.end_id,\n",
    "        'length': p.length,\n",
    "        'anchor_train_id': p.anchor_train_id,\n",
    "        'direction': p.direction,\n",
    "        'closing_train_id': p.closing_train_id,\n",
    "        'prev_train_id': p.prev_train_id,\n",
    "        'next_train_id': p.next_train_id,\n",
    "        'entry_inliers_forward': p.entry_inliers_forward,\n",
    "        'entry_inliers_backward': p.entry_inliers_backward,\n",
    "        'ordered_preview': str(p.ordered_test_ids[:6]) + (' ...' if len(p.ordered_test_ids) > 6 else ''),\n",
    "    })\n",
    "plan_df = pd.DataFrame(plan_rows)\n",
    "\n",
    "display(plan_df)\n",
    "\n",
    "# batch-step view (source frame -> target test frame)\n",
    "batch_rows = []\n",
    "for p in segment_plans:\n",
    "    prev_id = int(p.anchor_train_id)\n",
    "    prev_type = 'train'\n",
    "    for step_idx, tid in enumerate(p.ordered_test_ids):\n",
    "        batch_rows.append({\n",
    "            'segment_idx': p.segment_idx,\n",
    "            'step_idx': int(step_idx),\n",
    "            'direction': p.direction,\n",
    "            'source_id': int(prev_id),\n",
    "            'source_type': prev_type,\n",
    "            'target_id': int(tid),\n",
    "            'target_type': 'test',\n",
    "        })\n",
    "        prev_id = int(tid)\n",
    "        prev_type = 'test'\n",
    "batch_steps_df = pd.DataFrame(batch_rows)\n",
    "\n",
    "print('batch steps:', len(batch_steps_df))\n",
    "display(batch_steps_df.head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d126b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative localization + optional closure correction\n",
    "\n",
    "def choose_fallback_delta(direction: str) -> np.ndarray:\n",
    "    if str(direction) == 'backward':\n",
    "        return fallback_backward.copy()\n",
    "    return fallback_forward.copy()\n",
    "\n",
    "\n",
    "def predict_delta_with_fallback(m: PairMotion, direction: str, last_good_delta: Optional[np.ndarray]) -> Tuple[np.ndarray, str]:\n",
    "    if m.success:\n",
    "        return predict_delta(model, m), 'model'\n",
    "    if last_good_delta is not None:\n",
    "        return last_good_delta.copy(), 'last_good'\n",
    "    return choose_fallback_delta(direction), 'fallback_median'\n",
    "\n",
    "\n",
    "pred_map: Dict[int, np.ndarray] = {}\n",
    "step_diag_rows: List[Dict[str, object]] = []\n",
    "segment_diag_rows: List[Dict[str, object]] = []\n",
    "\n",
    "for p in segment_plans:\n",
    "    anchor_id = int(p.anchor_train_id)\n",
    "    anchor_pos = train_pos_map[anchor_id].astype(np.float64).copy()\n",
    "\n",
    "    segment_pred_local: Dict[int, np.ndarray] = {}\n",
    "    prev_id = anchor_id\n",
    "    prev_pos = anchor_pos.copy()\n",
    "    last_good_delta = None\n",
    "\n",
    "    for step_idx, tid in enumerate(p.ordered_test_ids):\n",
    "        m = estimate_pair_motion(prev_id, tid, SIFT_CFG)\n",
    "        dxy, mode = predict_delta_with_fallback(m, p.direction, last_good_delta)\n",
    "\n",
    "        cur_pos = prev_pos + dxy\n",
    "        segment_pred_local[int(tid)] = cur_pos\n",
    "\n",
    "        if mode == 'model':\n",
    "            last_good_delta = dxy.copy()\n",
    "\n",
    "        step_diag_rows.append({\n",
    "            'segment_idx': p.segment_idx,\n",
    "            'step_idx': int(step_idx),\n",
    "            'source_id': int(prev_id),\n",
    "            'target_id': int(tid),\n",
    "            'direction': p.direction,\n",
    "            'predict_mode': mode,\n",
    "            'matches': int(m.matches),\n",
    "            'affine_inliers': int(m.affine_inliers),\n",
    "            'sfm_inliers': int(m.sfm_inliers),\n",
    "            'pred_dx': float(dxy[0]),\n",
    "            'pred_dy': float(dxy[1]),\n",
    "            'pred_x': float(cur_pos[0]),\n",
    "            'pred_y': float(cur_pos[1]),\n",
    "        })\n",
    "\n",
    "        prev_id = int(tid)\n",
    "        prev_pos = cur_pos\n",
    "\n",
    "    closure_used = False\n",
    "    closure_err = np.array([0.0, 0.0], dtype=np.float64)\n",
    "    closure_inliers = 0\n",
    "\n",
    "    if (p.closing_train_id is not None) and (len(p.ordered_test_ids) > 0):\n",
    "        last_tid = int(p.ordered_test_ids[-1])\n",
    "        close_tid = int(p.closing_train_id)\n",
    "        m_close = estimate_pair_motion(last_tid, close_tid, SIFT_CFG)\n",
    "        closure_inliers = int(m_close.affine_inliers)\n",
    "\n",
    "        if m_close.success and (m_close.affine_inliers >= CLOSURE_MIN_INLIERS):\n",
    "            delta_close = predict_delta(model, m_close)\n",
    "            est_close = segment_pred_local[last_tid] + delta_close\n",
    "            gt_close = train_pos_map[close_tid]\n",
    "            closure_err = (gt_close - est_close).astype(np.float64)\n",
    "\n",
    "            n = len(p.ordered_test_ids)\n",
    "            for j, tid in enumerate(p.ordered_test_ids):\n",
    "                alpha = float(j + 1) / float(max(1, n))\n",
    "                segment_pred_local[int(tid)] = segment_pred_local[int(tid)] + alpha * closure_err\n",
    "\n",
    "            closure_used = True\n",
    "\n",
    "    for tid, pos in segment_pred_local.items():\n",
    "        pred_map[int(tid)] = pos.astype(np.float64)\n",
    "\n",
    "    segment_diag_rows.append({\n",
    "        'segment_idx': p.segment_idx,\n",
    "        'start_id': p.start_id,\n",
    "        'end_id': p.end_id,\n",
    "        'direction': p.direction,\n",
    "        'anchor_train_id': p.anchor_train_id,\n",
    "        'closing_train_id': p.closing_train_id,\n",
    "        'closure_used': bool(closure_used),\n",
    "        'closure_inliers': int(closure_inliers),\n",
    "        'closure_err_x': float(closure_err[0]),\n",
    "        'closure_err_y': float(closure_err[1]),\n",
    "        'closure_err_norm': float(np.linalg.norm(closure_err)),\n",
    "    })\n",
    "\n",
    "# sanity\n",
    "missing = [tid for tid in test_ids if int(tid) not in pred_map]\n",
    "if len(missing) > 0:\n",
    "    print('WARNING: missing predictions for ids:', missing[:20], '...')\n",
    "    for tid in missing:\n",
    "        prev_train = max([t for t in train_ids if t < int(tid)], default=min(train_ids))\n",
    "        pred_map[int(tid)] = train_pos_map[int(prev_train)].copy()\n",
    "\n",
    "segment_diag_df = pd.DataFrame(segment_diag_rows)\n",
    "step_diag_df = pd.DataFrame(step_diag_rows)\n",
    "\n",
    "print('predicted test frames:', len(pred_map), '/', len(test_ids))\n",
    "display(segment_diag_df)\n",
    "display(step_diag_df.head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build submission.csv\n",
    "sub_rows = []\n",
    "for tid in sorted(test_ids):\n",
    "    p = pred_map[int(tid)]\n",
    "    sub_rows.append({\n",
    "        'id': int(tid),\n",
    "        'x_pixel': float(p[0]),\n",
    "        'y_pixel': float(p[1]),\n",
    "    })\n",
    "submission_df = pd.DataFrame(sub_rows).sort_values('id').reset_index(drop=True)\n",
    "\n",
    "SUBMISSION_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "submission_df.to_csv(ALT_SUBMISSION_PATH, index=False)\n",
    "\n",
    "print('saved:', SUBMISSION_PATH)\n",
    "print('saved:', ALT_SUBMISSION_PATH)\n",
    "display(submission_df.head(20))\n",
    "display(submission_df.tail(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional map overlay diagnostics (train GT + test prediction)\n",
    "if MAP_PATH.exists():\n",
    "    map_bgr = cv2.imread(str(MAP_PATH), cv2.IMREAD_COLOR)\n",
    "    map_rgb = cv2.cvtColor(map_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    train_xy = np.array([train_pos_map[i] for i in train_ids], dtype=np.float64)\n",
    "    test_xy = submission_df[['x_pixel', 'y_pixel']].to_numpy(dtype=np.float64)\n",
    "\n",
    "    plt.figure(figsize=(13, 9))\n",
    "    plt.imshow(map_rgb)\n",
    "    plt.scatter(train_xy[:, 0], train_xy[:, 1], s=8, c='deepskyblue', alpha=0.45, label='train GT')\n",
    "    plt.scatter(test_xy[:, 0], test_xy[:, 1], s=14, c='red', alpha=0.8, label='test pred')\n",
    "    plt.title('Notebook 15 | Train GT + Test predictions')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('map.png not found, skipping overlay.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a721fa",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- This notebook intentionally keeps the base SIFT/SfM pipeline from notebook 12 (pre feature-focus sweep).\n",
    "- Segment planning follows your requested rule with the special reverse handling for IDs 1..12.\n",
    "- Drift correction uses optional closure to the next available train anchor.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
